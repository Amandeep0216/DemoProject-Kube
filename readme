SSO, or Single Sign-On, is a system that allows users to log in once and gain access to multiple applications without having to enter their credentials (like username and password) every time.

It saves time, improves security (because fewer passwords are needed), and makes managing access easier.

Benefits of SSO in DevOps:
	•	Convenience: Users don’t have to remember multiple passwords.
	•	Security: Fewer passwords to manage means fewer chances of them being compromised.
	•	Centralized Control: Administrators can easily manage and revoke access from a single point.


How to integrate SSO in DevOps:
	1	Choose an SSO provider: This could be something like Okta, Auth0, or Azure Active Directory.
	2	Configure Identity Providers (IdP): like Google or Microsoft
	3	Integrate with DevOps tools: You then link your SSO provider with DevOps tools (Jenkins, GitHub, Jira, etc.). Most modern DevOps tools have built-in support for SSO via protocols like SAML or OAuth.
	4	Set user roles and permissions: After integration, configure who has access to what, based on roles. For example, developers might have access to code repositories but not deployment systems.


PingOne: This is the identity provider (IdP) that stores and manages user credentials. 
Salesforce: This is an application that is configured to trust PingOne for authentication. Salesforce is the service provider (SP) in this case.
Login with Auth (SSO): By enabling the "Login with Auth" option in Salesforce, you're setting it up to allow users to log in via PingOne. Instead of entering a Salesforce-specific username and password, users will authenticate with PingOne (which is handling the credentials).


SAML: Security Assertion Markup Language
	•	What it is: SAML is an XML-based open standard used for exchanging authentication and authorization data between identity providers (like PingOne or AWS SSO) and service providers (like Salesforce or AWS). It allows you to log in once and access multiple applications securely.
OAuth: Open Authorization
	•	What it is: OAuth is an open standard for authorization. It allows a user to grant a third-party application limited access to their resources without giving away their credentials. OAuth is commonly used for enabling features like "Login with Google" or "Login with Facebook."





Windows Server is designed for handling networking, hosting, and data management tasks. It includes server-specific features like IIS (Internet Information Services), Active Directory, and Windows Deployment Services.


1. CMD (Command Prompt):
	•	Older Tool: CMD has been around since the early days of Windows.
2. PowerShell:
	•	Newer & More Powerful: PowerShell is a more advanced tool, designed to handle complex tasks and automation.
	•	Scripting Language: It is also a scripting language, which means you can write complex scripts to automate tasks.
	•	More Features: It can interact with the system more deeply, including managing network settings, services, processes, and much more.
	•	Objects, Not Just Text: Unlike CMD, PowerShell works with objects


Azure:-
Eligible = User can request and activate access to the role when needed.
Active = User already has access to the role and can use it right away.


Option 2: Create Alerts
Set the following details:
	•	Subscription: Choose your Azure subscription.
	•	Resource: Select the resource (e.g., a Virtual Machine).
	•	Condition: Choose a condition (e.g., CPU usage > 80%).
	•	Action: Choose the notification (email, webhook, etc.).



kubectl logs -f <pod-name>
kubectl logs -f <pod-name> -c <container-name>

New-Item -Path "C:\Path\To\Directory" -ItemType Directory
Set-Location -Path "C:\Path\To\Directory"



import boto3

# Initialize the EC2 resource
ec2 = boto3.resource('ec2')

# Create the EC2 instance
instances = ec2.create_instances(
    ImageId='ami-0c55b159cbfafe1f0',  # Example AMI ID, replace with your preferred AMI ID
    MinCount=1,  # Minimum number of instances
    MaxCount=1,  # Maximum number of instances
    InstanceType='t2.micro',  # Instance type
    KeyName='your-key-pair-name',  # Replace with your key pair name
)

# Output the instance IDs
for instance in instances:
    print(f'Instance {instance.id} created.')


python create_ec2.py


provider "azurerm" {
  features {}

  # Authentication credentials using environment variables or from Azure CLI (az login)
  client_id       = "<your-client-id>"
  client_secret   = "<your-client-secret>"
  tenant_id       = "<your-tenant-id>"
  subscription_id = "<your-subscription-id>"
}


# Define the Azure Virtual Machine
resource "azurerm_linux_virtual_machine" "example" {
  name                = "example-vm"
  resource_group_name = azurerm_resource_group.example.name
  location            = azurerm_resource_group.example.location
  size                = "Standard_DS1_v2"
  admin_username      = "azureuser"
  admin_password      = "P@ssw0rd1234"  # Secure password!
}

Terraform init
Terraform plan
Terraform apply
Terraform destroy

ssh azureuser@<public-ip-address>



Linux, windows server os
Azure and AWS
IAC terraform, Ansible, Docker, Kubernetes
Azure monitor, Splunk
CI/CD
Python, Powershell


















AWS / Azure

#study

EC2:-

Instance name,  AMI,  t2 micro,  key pair (.pem),  VPC to allow traffic, Storage config (RAM select),  IAM,  

Types of EC2:-

On-Demand: Pay for compute capacity by the hour/second.
Reserved Instances: Commit to a 1- or 3-year term for cost savings.
Spot Instances: Purchase unused capacity at discounted rates.
Savings Plans: Flexible pricing model for cost reduction.


S3:-

Bucket name (unique), Region,  Ownership (only you or others), Block public access,  bucket versioning, encryption 

To open object copy the object url

Create bucket policy to make the objects public and disable public access

Enable show versions option to enable the bucket versioning

To deploy static website go to static website hosting option and enable it and pastqe the index.html file.  One link will be generrated and use the same
	

IAM:-

go to users and click on create user, username, AWS credential type, Add permissions, Attach the permission 

Create group, enter group name and attach the policies


CloudWatch:-

Custom dashboard,  Automatic Dashboard

Cpu utilisation, network packets in/ out, disk usage  ,enable Alarm, 

Create Alarm, Alarm, Matric (ec2, s3), time, stats


EBS:-

SSD, HDD, Flexible sizes,  Backups, detachable


AWS Code commit, code build, code pipeline



Load Balancer:-

Classic, Application, Network, gateway

ALB :-
HTTP / HTTPs
Routes traffic based on content (e.g., URL, hostname, or headers

NLB :-

High-performance, low-latency traffic like gaming, video streaming, or financial systems.
* Routes traffic based on IP addresses and ports.

GLB :-

Deploying third-party appliances like firewalls, intrusion detection systems, and deep packet inspection.
* Acts as a gateway and a load balancer.

CLB :- 

Legacy applications or simple load balancing needs.
* Can route traffic for both HTTP/HTTPS (Layer 7) and TCP (Layer 4).




EC2 Template:-

Template name, AMI, instance type, key pairs, VPC to allow traffic, Storage config (RAM select),  IAM,



Create an Auto Scaling Group:

Name, Template select, VPC, Load balancers, 
Desired Capacity
Minimum Capacity
Maximum Capacity
Dynamic Scaling: Automatically add/remove instances based on demand
Add Notifications (Optional)








Blue/green deployments:-  means first will deploy to green env if everything works fine will deploy to blue env

Canary deployments:-  means first will deploy for small users 5-10% users if everything works fine will deploy for other users in canary state.





EC2 =  Azure VM
S3  =  Azure Blob
RDS  =  Azure SQL
IAM  =  Azure AD (Active Directory)







Azure VM:-

Subscription: Choose the appropriate subscription under which you want to create the VM.
Resource Group: Either select an existing resource group or create a new one.
Virtual Machine Name
Region
Availability Options: Select the availability option (e.g., "No infrastructure redundancy required" or choose a VM availability set or availability zone).
Image:
Size:select the number of CPUs, RAM, etc.
Authentication Type: Choose whether you want to use SSH public key (Linux) or password (Windows).


Virtual Network: Choose an existing virtual network or create a new one. This is the network your VM will be connected to.
Subnet: Select an existing subnet or create a new one.
Public IP:

Monitoring:
Auto-shutdown: Optionally, enable auto-shutdown for the VM at a specific time to save costs.
Backup:

OS Disk Type: Standard HDD, Standard SSD, or Premium SSD
Data Disks:

Review + Create





Azure blob:-

2. Create a Storage Account
* In the Azure portal, click on the "Create a resource" button located on the top left of the dashboard.
* Under the "Popular" category, select "Storage account".

3. Fill in Storage Account Details
* Subscription: Pay as you go
* Resource Group: Select an existing resource group or create a new one 
* Storage Account Name: unique name 
* Region: 
* Performance: Choose between Standard or Premium. Standard is typically sufficient for most use cases.
* Redundancy: Choose the redundancy option, such as Locally redundant storage (LRS), Geo-redundant storage (GRS), etc., based on your availability and redundancy requirements.

4. Review and Create
* After entering all required details, click on Next to proceed through the other settings like networking, data protection, etc. You can generally use the default options unless you have specific requirements.
* Finally, click on Review + Create to review your configuration, and then click on Create to provision the storage account.

5. Access the Blob Storage
* Once the storage account is created, navigate to it.
* In the left pane, select Containers under the Data Storage section.
* Click + Container to create a new container. A container is where you will store your blobs (objects like files).
* Choose a name for your container (names must be lowercase and can contain only letters, numbers, and hyphens).
* Set the container's access level: Private, Blob (anonymous read access for blobs only), or Container (anonymous read access to blobs and container metadata).
* After setting up the container, click OK.

6. Upload Files to Blob Storage
* After creating the container, you can upload files by clicking on the container name.
* Click Upload at the top of the container's page.
* Browse for the files you want to upload and select them. Then click Upload to store the files as blobs in your container.






Azure Active Directory:-


1. Create a New Directory:
    * In the Azure AD service pane, click on Create a directory.
    * You’ll be prompted to choose between two options:
        * Azure AD (Cloud-only): A cloud-only instance of Azure AD.
        * Azure AD B2C: For creating a customer identity and access management service.
2. For most scenarios, choose Azure AD (Cloud-only).
3. Fill in the Details:
    * Organization name: This is the name of your directory (e.g., "MyCompany").
    * Initial domain name: This will be the default domain for your directory (e.g., "mycompany.onmicrosoft.com").
    * Country or region: Select the country where your organization is located.
4. Review and Create:
    * After filling in the details, click on the Review + Create button.
    * The Azure portal will validate your entries.
    * If everything looks good, click Create to create the directory.

Step 3: Add Users and Groups to Your Directory
1. Add Users:
    * After the directory is created, you can start adding users.
    * In the Azure AD pane, select Users and then click + New user.
    * Choose between creating a new user or inviting an existing user.
    * Enter the required details, such as username, name, and role.
2. Add Groups (optional):
    * You can also create security groups for better user management.
    * In the Azure AD pane, click on Groups and then select + New group.
    * Define the group name, membership type (Assigned, Dynamic), and other options.
    * Add members to the group as needed.

Step 4: Assign Roles (Optional)
Azure AD allows you to assign roles to users for managing the directory and its resources:
1. In the Azure AD pane, go to Roles and administrators.
2. Select a role (e.g., Global Administrator, User Administrator) and click on Add assignment to assign the role to a user.

Step 5: Configure Other Azure AD Settings (Optional)
You can configure additional features in Azure AD:
* Custom Domains: You can add custom domains for your organization, like mycompany.com, by going to Custom domains under the Azure AD pane.
* Applications: You can register applications that can access Azure AD (e.g., single sign-on apps) via Enterprise applications.
* Security: Set up Multi-Factor Authentication (MFA), conditional access policies, and more under Security.

Step 6: Verify Your Directory
Once everything is set up, ensure that your Azure AD directory is working as expected by:
* Testing login for new users.
* Ensuring that roles and permissions are set correctly.
* Verifying that custom domains (if added) are working.
























Hi good afternoon,

#study 

My name is Aman deep i am from Bangalore I have done my graduation from invertis University in the field of computer science with specialization in cloud

My journey in the tech industry started with an enriching internship at JustDial, where I honed my skills as a DevOps Engineer trainee

And now I am working in infosys as senior associate consultant

Where Developed and maintained web applications using Java, Spring Boot, and Python, enhancing their functionality and performance.

Managed the scaling up and scaling down of EC2 instances based on the requirements, optimizing resource usage and reducing costs.

Created s3 buckets and load balancers to handle the load on the server and improve application uptime.

Created IAM roles for users and groups

Deployed the war file in Jenkins server by using ci/cd pipelines

Developed Python and Shell scripts for automation of the build and release process, reducing manual intervention and increasing efficiency.


////. Created docker containers and written groovy ci / cd pipeline script to automatically deploy the wars on the server by taking the pull from git


Built Docker containers to containerize applications, and wrote Groovy CI/CD pipeline scripts for automatic deployment from Git repositories.

Upgraded Linux servers deployed on EC2 instances, and updated for optimal performance.


Managed containerized applications using Kubernetes,
deploying and scaling services across clusters, and working with Minikube for local development and testing of Kubernetes pods.

In addition to my experience with AWS, I have also worked with Azure Virtual Machines (VMs), where I provisioned and managed VMs for scalable applications. 

Have also implemented Single Sign-On (SSO) solutions using Ping Identity, enabling seamless authentication across multiple applications.

Yeah soo these all are some of my roles and responsibilities

and on day to day basis we are using tools/ technologies like Jenkins, GIT, Linux, Shell, AWS, Docker 

Yeah that’s all about me
